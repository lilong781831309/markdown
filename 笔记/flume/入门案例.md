### 1、监控端口数据

配置文件 `netcat-memory-logger.conf`

```shell
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动

```shell
flume-ng agent --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf-file/netcat-memory-logger.conf --name a1 -Dflume.root.logger=INFO,console
```



#### 2、实时监控单个追加文件,将内容打印到控制台

配置文件 `exec-memory-logger.conf`

```shell
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /home/lilong/flume/exec.log

# Describe the sink
a1.sinks.k1.type = logger


# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动

```shell
flume-ng agent --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf-file/exec-memory-logger.conf --name a1 -Dflume.root.logger=INFO,console
```



#### 3.实时监控单个追加文件,将内容上传到HDFS中

配置文件 `exec-memory-hdfs.conf`

```shell
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /home/lilong/flume/exec.log

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoopcluster/flume/%Y-%m-%d/%H/%M
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = log
#上传文件的后缀
a1.sinks.k1.hdfs.inUseSuffix = .tmp
#是否按照时间滚动文件夹
a1.sinks.k1.hdfs.round = true
#多少时间单位创建一个新的文件夹
a1.sinks.k1.hdfs.roundValue = 1
#重新定义时间单位 second, minute or hour
#a1.sinks.k1.hdfs.roundUnit = hour
a1.sinks.k1.hdfs.roundUnit = minute
#是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次
#a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.batchSize = 5
#设置文件类型，可支持压缩
a1.sinks.k1.hdfs.fileType = DataStream
#多久生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 60
#设置每个文件的滚动大小
a1.sinks.k1.hdfs.rollSize = 134217700
#文件的滚动与Event数量无关
a1.sinks.k1.hdfs.rollCount = 0

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动

```shell
flume-ng agent --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf-file/exec-memory-hdfs.conf --name a1 -Dflume.root.logger=INFO,console
```



#### 4、实时监控目录下的新文件,将内容上传到HDFS中

配置文件 `spooldir-memory-hdfs.conf`

```shell
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /home/lilong/flume/spooldir
a1.sources.r1.fileSuffix = .COMPLETED
a1.sources.r1.ignorePattern = .*\.tmp$

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoopcluster/flume/%Y-%m-%d/%H/%M
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = log
#上传文件的后缀
a1.sinks.k1.hdfs.inUseSuffix = .tmp
#是否按照时间滚动文件夹
a1.sinks.k1.hdfs.round = true
#多少时间单位创建一个新的文件夹
a1.sinks.k1.hdfs.roundValue = 1
#重新定义时间单位 second, minute or hour
#a1.sinks.k1.hdfs.roundUnit = hour
a1.sinks.k1.hdfs.roundUnit = minute
#是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次
#a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.batchSize = 5
#设置文件类型，可支持压缩
a1.sinks.k1.hdfs.fileType = DataStream
#多久生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 60
#设置每个文件的滚动大小
a1.sinks.k1.hdfs.rollSize = 134217700
#文件的滚动与Event数量无关
a1.sinks.k1.hdfs.rollCount = 0

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动

```shell
flume-ng agent --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf-file/spooldir-memory-hdfs.conf --name a1 -Dflume.root.logger=INFO,console
```



#### 5、实时监控目录下的新文件,将内容上传到HDFS中

配置文件 `taildir-memory-hdfs.conf`

```shell
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = g1 g2
a1.sources.r1.filegroups.g1 = /home/lilong/flume/taildir/.*\.txt$
a1.sources.r1.filegroups.g2 = /home/lilong/flume/taildir/.*\.log$
a1.sources.r1.positionFile = /opt/module/flume/conf-file/taildir_position.json

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoopcluster/flume/%Y-%m-%d/%H/%M
#上传文件的前缀
a1.sinks.k1.hdfs.filePrefix = log
#上传文件的后缀
a1.sinks.k1.hdfs.inUseSuffix = .tmp
#是否按照时间滚动文件夹
a1.sinks.k1.hdfs.round = true
#多少时间单位创建一个新的文件夹
a1.sinks.k1.hdfs.roundValue = 1
#重新定义时间单位 second, minute or hour
#a1.sinks.k1.hdfs.roundUnit = hour
a1.sinks.k1.hdfs.roundUnit = minute
#是否使用本地时间戳
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次
#a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.batchSize = 5
#设置文件类型，可支持压缩
a1.sinks.k1.hdfs.fileType = DataStream
#多久生成一个新的文件
a1.sinks.k1.hdfs.rollInterval = 60
#设置每个文件的滚动大小
a1.sinks.k1.hdfs.rollSize = 134217700
#文件的滚动与Event数量无关
a1.sinks.k1.hdfs.rollCount = 0

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动

```shell
flume-ng agent --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/conf-file/taildir-memory-hdfs.conf --name a1 -Dflume.root.logger=INFO,console
```

